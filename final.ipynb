{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXUDiypkQI9k"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch sentencepiece datasets accelerate scikit-learn nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Import Core Libraries ---\n",
        "import torch\n",
        "import os\n",
        "import math\n",
        "import re # Used for potential text cleaning, though minimal in this version\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from sklearn.model_selection import train_test_split # Used for splitting data for evaluation\n",
        "from torch.utils.data import Dataset # Base class for custom datasets\n",
        "\n",
        "# Hugging Face Transformers specific imports\n",
        "from transformers import (\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TextDataset # Note: TextDataset is a legacy class, CustomTextDataset is preferred for flexibility\n",
        ")\n"
      ],
      "metadata": {
        "id": "29tfFGskSYKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Hardware Setup: Verify GPU Availability ---\n",
        "# Check if a CUDA-enabled GPU is available and set the device accordingly.\n",
        "# This ensures the model training and inference runs on GPU if possible, for speed.\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "0Lxx20ifSYHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Data Collection and Preparation ---\n",
        "# Download the 'gutenberg' corpus from NLTK, which contains Jane Austen's 'Emma'.\n",
        "print(\"Downloading NLTK 'gutenberg' corpus...\")\n",
        "nltk.download('gutenberg')\n"
      ],
      "metadata": {
        "id": "86M_eoeUSYE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the raw text data for 'austen-emma.txt' (Emma) from the Gutenberg corpus.\n",
        "# This loads the text including any Project Gutenberg boilerplate.\n",
        "data = gutenberg.raw('austen-emma.txt')\n"
      ],
      "metadata": {
        "id": "1N1U333iSYC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple cleaning function to remove Project Gutenberg boilerplate.\n",
        "# This helps the model focus on the actual narrative.\n",
        "def clean_gutenberg_text(text):\n",
        "    # Markers to identify the start and end of the actual book content\n",
        "    start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK EMMA ***\"\n",
        "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK EMMA ***\"\n",
        "\n",
        "    # Find the start and end indices of the book content\n",
        "    start_idx = text.find(start_marker)\n",
        "    if start_idx != -1:\n",
        "        text = text[start_idx + len(start_marker):] # Slice from after the start marker\n",
        "\n",
        "    end_idx = text.find(end_marker)\n",
        "    if end_idx != -1:\n",
        "        text = text[:end_idx] # Slice up to the end marker\n",
        "\n",
        "    # Standardize newlines and remove excess spaces\n",
        "    text = text.replace('\\r\\n', '\\n') # Replace Windows-style newlines with Unix-style\n",
        "    text = re.sub(r'\\n\\s*\\n', ' ', text) # Replace multiple newlines with a single space\n",
        "    text = text.replace('\\n', ' ')       # Replace single newlines with a single space\n",
        "    text = ' '.join(text.split())        # Remove any extra spaces and standardize spacing\n",
        "\n",
        "    # Remove common Gutenberg artifacts (e.g., illustration tags)\n",
        "    text = re.sub(r'\\[Illustration:.*?\\]', '', text)\n",
        "    text = re.sub(r'\\[.*?\\]', '', text) # General catch-all for bracketed notes\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the raw Emma text\n",
        "cleaned_emma_text = clean_gutenberg_text(data)\n",
        "print(f\"Original Emma text length: {len(data)} characters\")\n",
        "print(f\"Cleaned Emma text length: {len(cleaned_emma_text)} characters\")\n"
      ],
      "metadata": {
        "id": "wLzqGwNXSYAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the cleaned text to a local file. This file will be used by TextDataset.\n",
        "train_file_path = 'emma_cleaned.txt' # Changed filename to indicate it's cleaned\n",
        "with open(train_file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(cleaned_emma_text)\n",
        "print(f\"Cleaned text saved to: {train_file_path}\")"
      ],
      "metadata": {
        "id": "YuLtJ-zGSX-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Custom Dataset for Evaluation ---\n",
        "# Define a custom PyTorch Dataset for handling text data.\n",
        "# This is more flexible than TextDataset, especially for splitting data or\n",
        "# when data is already in memory.\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, text_data, block_size):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # Tokenize the entire text. Add EOS token at the very end for consistent sequence ending.\n",
        "        # truncation=False and padding=False mean we handle chunking manually.\n",
        "        tokenized_data = tokenizer(text_data + tokenizer.eos_token, return_tensors='pt', truncation=False, padding=False)\n",
        "        self.input_ids = tokenized_data['input_ids'][0] # Get the tensor of token IDs\n",
        "\n",
        "        # Ensure we only work with full blocks. Discard any leftover tokens at the end.\n",
        "        self.num_blocks = len(self.input_ids) // self.block_size\n",
        "        self.input_ids = self.input_ids[:self.num_blocks * self.block_size]\n",
        "\n",
        "        print(f\"Dataset initialized with {len(self.input_ids)} total tokens, split into {self.num_blocks} blocks of size {self.block_size}.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        # The length of the dataset is the number of full blocks\n",
        "        return self.num_blocks\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return a single block of tokens and its corresponding labels.\n",
        "        # For causal language modeling (CLM), labels are the same as input_ids,\n",
        "        # as the model predicts the next token in the sequence.\n",
        "        chunk = self.input_ids[idx * self.block_size : (idx + 1) * self.block_size]\n",
        "        return {\"input_ids\": chunk, \"labels\": chunk.clone()}\n"
      ],
      "metadata": {
        "id": "Si3OCPSRS4Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Data Loading and Collator Functions for Training ---\n",
        "# These functions are used by the Hugging Face Trainer.\n",
        "\n",
        "# Function to load the dataset using TextDataset (legacy but used in original code)\n",
        "def load_dataset_for_training(file_path, tokenizer, block_size=128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "# Function to load the Data Collator for Language Modeling.\n",
        "# mlm=False indicates Causal Language Modeling (CLM) for GPT-2,\n",
        "# where the model predicts the next token in a sequence.\n",
        "def load_data_collator(tokenizer, mlm=False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator\n"
      ],
      "metadata": {
        "id": "r0m_5XQSS4OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "6dGSaxObW2EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o_M2WhDHXPtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset_for_training(train_file_path, tokenizer)"
      ],
      "metadata": {
        "id": "pAEblBs5XCNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = load_data_collator(tokenizer)"
      ],
      "metadata": {
        "id": "IbYWkV66Yk1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator"
      ],
      "metadata": {
        "id": "PVstbIRBYyV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Function ---\n",
        "# Encapsulates the entire fine-tuning process.\n",
        "def train_model(train_file_path, model_name,\n",
        "              output_dir,\n",
        "              overwrite_output_dir,\n",
        "              per_device_train_batch_size,\n",
        "              num_train_epochs,\n",
        "              save_steps): # save_steps is passed but not used in TrainingArguments here\n",
        "\n",
        "    # Initialize tokenizer from pre-trained model\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    # Load the training dataset using the specified file path and tokenizer\n",
        "    train_dataset = load_dataset_for_training(train_file_path, tokenizer)\n",
        "    # Load the data collator\n",
        "    data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "    # Save the tokenizer to the output directory. This is important for loading later.\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    # Set pad_token to eos_token for GPT-2, crucial for batching and generation\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load the pre-trained GPT-2 model and move it to the specified device (GPU/CPU)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "    # Ensure model's pad_token_id matches tokenizer's\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "    # Save the initial (untrained) model to the output directory.\n",
        "    # This line is redundant if trainer.save_model() is called later,\n",
        "    # but kept as per original code.\n",
        "    model.save_pretrained(output_dir)\n",
        "\n",
        "    # Configure training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=overwrite_output_dir,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        # No explicit eval_dataset or evaluation_strategy in original train function\n",
        "        # Logging and saving steps are also not explicitly set here but are in the outer call\n",
        "        logging_steps=100, # Default logging frequency\n",
        "        save_steps=save_steps, # Default saving frequency\n",
        "        report_to=\"wandb\" # Report metrics to Weights & Biases\n",
        "    )\n",
        "\n",
        "    # Initialize the Hugging Face Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "    )\n",
        "\n",
        "    # Start the training process\n",
        "    print(\"Starting model training...\")\n",
        "    trainer.train()\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    # Save the final trained model\n",
        "    trainer.save_model()\n",
        "    print(f\"Fine-tuned model saved to {output_dir}\")"
      ],
      "metadata": {
        "id": "4mrmCJ8pS4Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E25AMLNlWwyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Set Training Parameters and Execute Training ---\n",
        "# Define the specific parameters for this training run.\n",
        "model_name = 'gpt2'\n",
        "output_dir = './fine_tuned_gpt2_alice' # Output directory for the fine-tuned model\n",
        "overwrite_output_dir = False # Set to False as per original notebook\n",
        "per_device_train_batch_size = 4\n",
        "num_train_epochs = 5\n",
        "save_steps = 500 # This parameter is passed to the train function\n"
      ],
      "metadata": {
        "id": "Hw_RkitsS4JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Weights & Biases API key for logging (if using WandB)\n",
        "os.environ[\"WANDB_API_KEY\"] = \"ff4e8f8113901dcad2887ab7d4459b8cdc5e62f9\"\n",
        "\n",
        "# Execute the training function\n",
        "train_model(\n",
        "    train_file_path=train_file_path, # Path to the cleaned Emma text file\n",
        "    model_name=model_name,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=overwrite_output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps # Pass save_steps to the function\n",
        ")\n"
      ],
      "metadata": {
        "id": "jPGkuGErVbym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 9. Text Generation Functionality ---\n",
        "# Functions to load the model and tokenizer for text generation.\n",
        "def load_model_for_generation(model_path):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "def load_tokenizer_for_generation(tokenizer_path):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
        "    return tokenizer\n",
        "\n",
        "# Function to generate text using the fine-tuned model.\n",
        "def generate_text(model_path, sequence, max_length):\n",
        "    # Load model and tokenizer within the function (less efficient for multiple calls)\n",
        "    model = load_model_for_generation(model_path)\n",
        "    tokenizer = load_tokenizer_for_generation(model_path)\n",
        "\n",
        "    # Ensure tokenizer has pad_token defined and model.config.eos_token_id is set\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Move model to GPU if available and set to evaluation mode\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Encode the input sequence to token IDs\n",
        "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt').to(device)\n",
        "\n",
        "    # Generate text using various sampling strategies\n",
        "    final_outputs = model.generate(\n",
        "        ids,\n",
        "        do_sample=True, # Enable sampling for more diverse outputs\n",
        "        max_length=max_length, # Maximum length of the generated sequence (including prompt)\n",
        "        pad_token_id=model.config.eos_token_id, # Token to use for padding\n",
        "        top_k=50, # Consider only the top 50 most likely tokens\n",
        "        top_p=0.95, # Nucleus sampling: consider tokens whose cumulative probability is 0.95\n",
        "        # repetition_penalty=1.2, # Optional: Penalize repeated tokens (can be added for more control)\n",
        "    )\n",
        "\n",
        "    # Decode the generated token IDs back to human-readable text\n",
        "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "op7VQWLmewJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 10. Example Text Generation ---\n",
        "# Set the model path to the output directory where the fine-tuned model is saved.\n",
        "model_path_for_generation = output_dir\n",
        "sequence_to_generate = \"Emma, in her confusion, declared that\"\n",
        "max_generation_length = 50 # Total length including the prompt\n",
        "\n",
        "# Generate text using the fine-tuned model\n",
        "print(\"\\n--- Generating Text Sample ---\")\n",
        "generate_text(model_path_for_generation, sequence_to_generate, max_generation_length)\n"
      ],
      "metadata": {
        "id": "vDsMtWMXfGNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 11. Save Model to Google Drive ---\n",
        "# Mount Google Drive to save the trained model persistently.\n",
        "# This requires user authentication in Colab.\n",
        "print(\"\\n--- Mounting Google Drive ---\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "JbPS3soBfGKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths for copying the model to Google Drive.\n",
        "colab_model_output_dir = '/content/fine_tuned_gpt2_alice' # Source directory in Colab\n",
        "# Destination directory in Google Drive. Using a date for unique naming.\n",
        "drive_save_dir = '/content/drive/MyDrive/MyNLPModels/EmmaGPT2_model_10July2025'\n"
      ],
      "metadata": {
        "id": "WEhrfnWmfjwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "# Create the target directory in Google Drive if it doesn't already exist.\n",
        "os.makedirs(drive_save_dir, exist_ok=True)\n",
        "\n",
        "# Copy the entire fine-tuned model directory from Colab's local storage to Google Drive.\n",
        "# `dirs_exist_ok=True` prevents error if directory already exists.\n",
        "print(f\"Copying model from {colab_model_output_dir} to {drive_save_dir}...\")\n",
        "try:\n",
        "    shutil.copytree(colab_model_output_dir, drive_save_dir, dirs_exist_ok=True)\n",
        "    print(f\"Model successfully saved to Google Drive at: {drive_save_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving model to Google Drive: {e}\")\n"
      ],
      "metadata": {
        "id": "6xknCjckfjsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 12. Quantitative Evaluation: Perplexity Calculation ---\n",
        "# This section performs the evaluation of the trained model on the full dataset\n",
        "# to calculate perplexity.\n",
        "\n",
        "# Load the raw Emma text data for evaluation.\n",
        "# Note: This uses the raw text, as per your instruction to not add more cleaning.\n",
        "# If you prefer to evaluate on the *cleaned* text, use `cleaned_emma_text` here.\n",
        "emma_raw_text_for_eval = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "# Initialize tokenizer and model for evaluation.\n",
        "# IMPORTANT: Ensure `model_load_path_eval` points to your fine-tuned model.\n",
        "model_load_path_eval = '/content/fine_tuned_gpt2_alice' # Should match output_dir\n",
        "\n",
        "tokenizer_eval = GPT2Tokenizer.from_pretrained(model_load_path_eval)\n",
        "model_eval = GPT2LMHeadModel.from_pretrained(model_load_path_eval)\n",
        "\n",
        "# Ensure tokenizer has pad_token defined and model.config.pad_token_id is set\n",
        "if tokenizer_eval.pad_token is None:\n",
        "    tokenizer_eval.pad_token = tokenizer_eval.eos_token\n",
        "model_eval.config.pad_token_id = tokenizer_eval.pad_token_id\n",
        "\n",
        "model_eval.to(device) # Move model to GPU if available\n",
        "model_eval.eval() # Set model to evaluation mode\n",
        "\n",
        "print(f\"\\nModel and tokenizer loaded for evaluation from: {model_load_path_eval}\")\n",
        "\n",
        "# Prepare the Evaluation Dataset using the CustomTextDataset.\n",
        "# Use the same block_size that was used during training.\n",
        "block_size_eval = 128 # Must match training block_size\n",
        "\n",
        "# Create the evaluation dataset using the entire raw text.\n",
        "eval_dataset = CustomTextDataset(tokenizer=tokenizer_eval, text_data=emma_raw_text_for_eval, block_size=block_size_eval)\n",
        "\n",
        "# Data Collator for Language Modeling (necessary for batching during evaluation)\n",
        "data_collator_eval = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer_eval,\n",
        "    mlm=False # Set to False for Causal Language Modeling (like GPT-2)\n",
        ")\n",
        "\n",
        "# Set up Trainer for Evaluation.\n",
        "# We need minimal TrainingArguments for this evaluation pass.\n",
        "eval_args = TrainingArguments(\n",
        "    output_dir=\"./evaluation_results\", # A temporary directory for evaluation logs\n",
        "    per_device_eval_batch_size=4,       # Use a suitable batch size for evaluation\n",
        "    do_eval=True,                       # Indicate that we are doing evaluation\n",
        "    report_to=\"none\",                   # No external reporting needed for this simple evaluation\n",
        ")\n",
        "\n",
        "trainer_eval = Trainer(\n",
        "    model=model_eval,\n",
        "    args=eval_args,\n",
        "    eval_dataset=eval_dataset, # Pass the evaluation dataset\n",
        "    data_collator=data_collator_eval,\n",
        ")\n",
        "\n",
        "# Run Evaluation and Calculate Perplexity.\n",
        "print(\"\\n--- Starting Model Evaluation (Perplexity Calculation) ---\")\n",
        "# The `evaluate` method will compute the loss on the eval_dataset\n",
        "eval_results = trainer_eval.evaluate()\n",
        "\n",
        "# Extract the evaluation loss from the results\n",
        "eval_loss = eval_results[\"eval_loss\"]\n",
        "\n",
        "# Calculate Perplexity: Perplexity = exp(loss)\n",
        "perplexity = math.exp(eval_loss)\n",
        "\n",
        "print(f\"Evaluation Loss on 'emma.txt' (raw): {eval_loss:.4f}\")\n",
        "print(f\"Perplexity on 'emma.txt' (raw): {perplexity:.2f}\")\n",
        "print(\"Full Evaluation results:\", eval_results)\n",
        "print(\"\\n--- Evaluation Complete ---\")"
      ],
      "metadata": {
        "id": "lR-6VywLfjqs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}